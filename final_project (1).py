# -*- coding: utf-8 -*-
"""Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bje4lKAUmdNINKNMFnMMzyU8dbIk1X3q
"""

import pandas as pd
import json
import ast

file_path = '/content/qa_Cell_Phones_and_Accessories.json'

data = []
# Read the JSON Lines file line by line
with open(file_path, 'r') as file:
    for line in file:
        try:
            line_dict = ast.literal_eval(line.strip())
            data.append(line_dict)
        except (SyntaxError, ValueError) as e:
            print(f"Error decoding line: {line.strip()}")
            print(f"Error: {e}")

# Convert the list of JSON objects to a DataFrame
df = pd.DataFrame(data)
print(df.head())
print(df.dtypes)

# Convert the list of JSON objects to a DataFrame
df = pd.DataFrame(data)

# Display the first few rows of the DataFrame
print(df.head())

# Check data types
print(df.dtypes)

# Convert answerTime to datetime
df['answerTime'] = pd.to_datetime(df['answerTime'], errors='coerce')

# Check for missing values
print(df.isnull().sum())

# Drop rows where answerTime is missing
df = df.dropna(subset=['answerTime'])

# Handling missing values (example: filling NaNs in answerType with 'Unknown')
df['answerType'] = df['answerType'].fillna('Unknown')

# Recheck for missing values
print(df.isnull().sum())

# Save the DataFrame to a CSV file for future analysis
df.to_csv('/content/qa_Cell_Phones_and_Accessories_cleaned.csv', index=False)

# Display summary statistics
print(df.describe(include='all'))

# Verify the data types after conversions
print(df.dtypes)

# Display the first few rows of the cleaned DataFrame
print(df.head())

import spacy
import pandas as pd
import numpy as np
import multiprocessing as mp
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk

# Download NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Load spaCy model
nlp = spacy.load('en_core_web_sm')

# Function to preprocess text
def preprocess_text(text):
    doc = nlp(text.lower())
    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]
    return ' '.join(tokens)

# Function to apply preprocessing in parallel
def parallelize_dataframe(df, func, n_cores=4):
    df_split = np.array_split(df, n_cores)
    pool = mp.Pool(n_cores)
    df = pd.concat(pool.map(func, df_split))
    pool.close()
    pool.join()
    return df

# Apply preprocessing to each row
def preprocess_text_dataframe(df):
    df['cleaned_question'] = df['question'].apply(preprocess_text)
    df['cleaned_answer'] = df['answer'].apply(preprocess_text)
    return df

# Load your DataFrame here
df = pd.read_csv('/content/qa_Cell_Phones_and_Accessories_cleaned.csv')

# Apply parallel preprocessing
df = parallelize_dataframe(df, preprocess_text_dataframe)

# Save preprocessed data
df.to_csv('/content/qa_Cell_Phones_and_Accessories_preprocessed.csv', index=False)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import traifrom sklearn.svm import SVC
from sklearn.metrics import classification_report

# Vectorize text data
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df['cleaned_question'])
y = df['questionType']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train SVM model
svm_model = SVC()
svm_model.fit(X_train, y_train)

# Evaluate model
y_pred = svm_model.predict(X_test)
print(classification_report(y_test, y_pred))

from transformers import BertTokenizer, BertForTokenClassification, Trainer, TrainingArguments
from datasets import load_metric, Dataset

# Load pre-trained BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Function to tokenize data
def tokenize_function(examples):
    return tokenizer(examples['cleaned_question'], padding="max_length", truncation=True)

# Create Dataset object
dataset = Dataset.from_pandas(df[['cleaned_question', 'cleaned_answer']])
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Load pre-trained BERT model
model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Training arguments
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test'],
)

# Train and evaluate model
trainer.train()
trainer.evaluate()

from sklearn.metrics import precision_score, recall_score, f1_score

# Calculate metrics for SVM model
svm_precision = precision_score(y_test, y_pred, average='weighted')
svm_recall = recall_score(y_test, y_pred, average='weighted')
svm_f1 = f1_score(y_test, y_pred, average='weighted')

print(f"SVM Precision: {svm_precision}")
print(f"SVM Recall: {svm_recall}")
print(f"SVM F1-Score: {svm_f1}")